{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "#grid search to find optimal parameters\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Dimension reduction and clustering libraries\n",
    "import umap\n",
    "import hdbscan\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_target_word(context, positions):\n",
    "    positions = [i.split('-') for i in positions.split(',')]\n",
    "    for position in positions:\n",
    "        start = int(position[0])\n",
    "        end = int(position[1])\n",
    "        return context.replace(context[start:end], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_preprocess(text):\n",
    "    result = []\n",
    "    for sentence in text.split('.'):\n",
    "        tokens = [re.sub('[^А-Яа-я.!? ]', ' ', token) for token in word_tokenize(sentence) if token not in russian_stopwords\\\n",
    "              and token != \" \" and token != \"\" \\\n",
    "              and token.strip() not in punctuation]\n",
    "        result.append(tokens)\n",
    "    return [x for x in result if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocess(text):\n",
    "    result = []\n",
    "    for sentence in text.split('.'):\n",
    "        tokens = [re.sub('[^А-Яа-я.!? ]', ' ', token) for token in word_tokenize(sentence) if token not in russian_stopwords\\\n",
    "              and token != \" \" and token != \"\" \\\n",
    "              and token.strip() not in punctuation]\n",
    "        sentence = ' '.join(tokens)\n",
    "        result.append(sentence)\n",
    "    return [x for x in result if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем модель ELMO\n",
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\",\\\n",
    "                    elmo_output_names=[\"elmo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем конфиги руберта\n",
    "bert_config = read_json(configs.embedder.bert_embedder)\n",
    "bert_config['metadata']['download'][0]['url'] = 'http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz'\n",
    "bert_config['metadata']['variables']['BERT_PATH'] = '{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем модель BERT\n",
    "bert = build_model(bert_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_vectors(bert_context_vectors, context, positions):\n",
    "    words = []\n",
    "    vectors = []\n",
    "    positions = [i.split('-') for i in positions.split(',')]\n",
    "    for position in positions:\n",
    "        start = int(position[0])\n",
    "        end = int(position[1])\n",
    "        words.append(context[start:end+1])\n",
    "    for word in words:\n",
    "        for list_idx, token_list in enumerate(bert_context_vectors[0]):\n",
    "            if word in token_list:\n",
    "                word_idx = token_list.index(word)\n",
    "                bert_token_vector = bert_context_vectors[1][list_idx][word_idx]\n",
    "                vectors.append(bert_token_vector)\n",
    "    try:\n",
    "        result = np.vstack(vectors)\n",
    "    except ValueError:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectorize(elmo_preprocessed_text):\n",
    "    return elmo(elmo_preprocessed_text)\n",
    "\n",
    "def bert_vectorize(bert_preprocessed_text):\n",
    "    return(bert(bert_preprocessed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pooler_outputs_mean(vectors):\n",
    "        \"\"\" Function to average BERT pooler outputs \"\"\"\n",
    "    average_vector = np.array([sum(subvector) / len(subvector) for subvector in vectors[6].transpose()])\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_vectors(bert_embeddings):\n",
    "    vectors = []\n",
    "    for i, vector in enumerate(bert_embeddings):\n",
    "        vectors.append(vector[0])\n",
    "        \n",
    "    X = np.vstack(vectors)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "standard_embedding = reducer.fit_transform(X)\n",
    "standard_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [sns.color_palette()[x] for x in X_gold_senses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=[sns.color_palette()[x] for x in X_gold_senses], cmap='Spectral')\n",
    "\n",
    "# plt.title(\"UMAP projection of BERT embeddings of word 'белок'\", fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parameters(X):\n",
    "    \"\"\" Function to find optimal parameters for Affinity propagation algorithm\"\"\"\n",
    "    S =-pairwise_distances(X,metric=affinity, squared=True)\n",
    "    param_grid   =  np.unique(map(int, np.linspace(np.min(S), np.median(S), 30)))\n",
    "    search = GridSearchCV(AffinityPropagation(), param_grid, verbose=0) \n",
    "    return search.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_via_affinity_prop(matrix, gold_senses=list_of_dom_gold_senses, preference=preference):\n",
    "    \"\"\" Clustering via Affinity propagation algorithm \"\"\"\n",
    "    af = AffinityPropagation(preference=preference).fit(matrix)\n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    labels = af.labels_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(gold_senses, labels))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(gold_senses, labels))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(gold_senses, labels))\n",
    "    print(\"Adjusted Rand Index: %0.3f\"\n",
    "          % metrics.adjusted_rand_score(gold_senses, labels))\n",
    "    print(\"Adjusted Mutual Information: %0.3f\"\n",
    "          % metrics.adjusted_mutual_info_score(gold_senses, labels))\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(matrix, labels, metric='sqeuclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_word_senses(df, preference):\n",
    "\n",
    "    predicted = []\n",
    "    gold_senses = []\n",
    "    for query in df.word.unique():\n",
    "        print('Now analyzing', query, '...')\n",
    "        df_word = df[df['word'] == query]\n",
    "        vectors = []\n",
    "        for i, vector in enumerate(df_word.target_words_bert_embs):\n",
    "            vectors.append(vector[0])\n",
    "        X = np.vstack(vectors)\n",
    "        print('emb shape: ',X.shape)\n",
    "        X_gold_senses = [int(i) for i in df_word.gold_sense_id]\n",
    "        min_gold = min(X_gold_senses)\n",
    "        X_gold_senses = [i - min_gold for i in X_gold_senses]\n",
    "        print('gold', X_gold_senses)\n",
    "        print('len gold_senses: ', len(X_gold_senses))\n",
    "        gold_senses += X_gold_senses\n",
    "        gold_senses = [str(i) for i in gold_senses]\n",
    "        \n",
    "        clustering = AffinityPropagation(preference=preference).fit(X)\n",
    "        cur_predicted = clustering.labels_.tolist()\n",
    "        predicted += cur_predicted\n",
    "        predicted = [str(i) for i in predicted]\n",
    "    \n",
    "    df.predict_sense_id = predicted\n",
    "    df.gold_senses = gold_senses\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ari_per_word_weighted(df):\n",
    "    \"\"\" Function for final evaluation. \"\"\"\n",
    "    \n",
    "    words = {word: (adjusted_rand_score(df_word.gold_sense_id, df_word.predict_sense_id), len(df_word))\n",
    "             for word in df.word.unique()\n",
    "             for df_word in (df.loc[df['word'] == word],)}\n",
    "    \n",
    "    print(words)\n",
    "\n",
    "    cumsum = sum(ari * count for ari, count in words.values())\n",
    "    total  = sum(count for _, count in words.values())\n",
    "    \n",
    "    print(cumsum, total)\n",
    "\n",
    "    assert total == len(df), \n",
    "\n",
    "    return cumsum / total, words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
